{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c5ecf9",
   "metadata": {},
   "source": [
    "ETL Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e9f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necesary header files and path set\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_232-cloudera/jre\"\n",
    "os.environ[\"SPARK_HOME\"]=\"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec68f29",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-46a638a2fb45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'jupyter_Spark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('jupyter_Spark').master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data from the SRC_ATM_TRANS table to spark\n",
    "df = spark.read.csv(\"/user/root/SRC_ATM_TRANS/part-m-00000\", header = False, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the dataframe created above\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63567876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom input schema using StrucType\n",
    "#import necessary header\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba3856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema = StructType([StructField('year', IntegerType(), nullable = True),\n",
    "                        StructField('month', StringType(), True),\n",
    "                        StructField('day', IntegerType(), True),\n",
    "                        StructField('weekday', StringType(), True),\n",
    "                        StructField('hour', IntegerType(), True),\n",
    "                        StructField('atm_status', StringType(), True),\n",
    "                        StructField('atm_id', StringType(), True),\n",
    "                        StructField('atm_manufacturer', StringType(), True),\n",
    "                        StructField('atm_location', StringType(), True),\n",
    "                        StructField('atm_streetname', StringType(), True),\n",
    "                        StructField('atm_street_number', IntegerType(), True),\n",
    "                        StructField('atm_zipcode', IntegerType(), True),\n",
    "                        StructField('atm_lat', DoubleType(), True),\n",
    "                        StructField('atm_lon', DoubleType(), True),\n",
    "                        StructField('currency', StringType(), True),\n",
    "                        StructField('card_type', StringType(), True),\n",
    "                        StructField('transaction_amount', IntegerType(), True),\n",
    "                        StructField('service', StringType(), True),\n",
    "                        StructField('message_code', StringType(), True),\n",
    "                        StructField('message_text', StringType(), True),\n",
    "                        StructField('weather_lat', DoubleType(), True),\n",
    "                        StructField('weather_lon', DoubleType(), True),\n",
    "                        StructField('weather_city_id', IntegerType(), True),\n",
    "                        StructField('weather_city_name', StringType(), True),\n",
    "                        StructField('temp', DoubleType(), True),\n",
    "                        StructField('pressure', IntegerType(), True),\n",
    "                        StructField('humidity', IntegerType(), True),\n",
    "                        StructField('wind_speed', IntegerType(), True),\n",
    "                        StructField('wind_deg', IntegerType(), True),\n",
    "                        StructField('rain_3h', DoubleType(), True),\n",
    "                        StructField('clouds_all', IntegerType(), True),\n",
    "                        StructField('weather_id', IntegerType(), True),\n",
    "                        StructField('weather_main', StringType(), True),\n",
    "                        StructField('weather_description', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the SRC_ATM_TRANS data in the dataframe\n",
    "df = spark.read.csv(\"/user/root/SRC_ATM_TRANS/part-m-00000\", header = False, schema = Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d57b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of records loaded in the df\n",
    "df.select('*').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cdfdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8bd12",
   "metadata": {},
   "source": [
    "Creating Fact and Dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating distinct df and selecting required columns\n",
    "location = df.select('atm_location', 'atm_streetname', 'atm_street_number', 'atm_zipcode', 'atm_lat', 'atm_lon').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867eb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary headers\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the primary key\n",
    "df_temp = location.rdd.zipWithIndex().toDF()\n",
    "dim_location = df_temp.select(col(\"_1.*\"),col(\"_2\").alias('location_id'))\n",
    "dim_location.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column as required \n",
    "DIM_LOCATION = dim_location.withColumnRenamed('atm_location','location').withColumnRenamed('atm_streetname','streetname').withColumnRenamed('atm_street_number','street_number').withColumnRenamed('atm_zipcode','zipcode').withColumnRenamed('atm_lat','lat').withColumnRenamed('atm_lon','lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rechecking the column names \n",
    "DIM_LOCATION.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary df for the dimension atm and selecting required columns\n",
    "atm = df.select('atm_id', 'atm_manufacturer', 'atm_lat', 'atm_lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b41806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column as required \n",
    "atm = atm.withColumnRenamed('atm_id', 'atm_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53019d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the dim_location and atm dataframes\n",
    "atm = atm.join(dim_location, on = ['atm_lat', 'atm_lon'], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to columns in the joined df\n",
    "atm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef730b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the required columns and making sure records are distinct\n",
    "atm = atm.select('atm_number', 'atm_manufacturer', 'location_id').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2529a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the column as required \n",
    "atm = atm.withColumnRenamed('location_id', 'atm_location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing changes in columns\n",
    "atm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8340ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the primary key column\n",
    "df_temp = atm.rdd.zipWithIndex().toDF()\n",
    "dim_atm = df_temp.select(col(\"_1.*\"),col(\"_2\").alias('atm_id'))\n",
    "dim_atm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85682391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearranging the columns according to the target\n",
    "DIM_ATM = dim_atm.select('atm_id', 'atm_number', 'atm_manufacturer', 'atm_location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all required columns are present and named correctly\n",
    "DIM_ATM.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cb899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df date and selecting required columns\n",
    "date = df.select('year', 'month', 'day', 'hour', 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date.withColumn('full_date', concat_ws('-', date.year, date.month, date.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6aa2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date.withColumn('full_time', concat_ws(':', date.hour, lit('00'), lit('00')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88866ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date.withColumn('full_date_time', concat_ws(' ', date.full_date, date.full_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting date \n",
    "pattern = 'yyyy-MMM-dd HH:mm:ss'\n",
    "date = date.withColumn('full_date_time', unix_timestamp(date.full_date_time, pattern).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89001620",
   "metadata": {},
   "outputs": [],
   "source": [
    "date.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the required columns and make it distinct\n",
    "date = date.select('year', 'month', 'day', 'hour', 'weekday', 'full_date_time').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc02336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the primary key column\n",
    "df_temp = date.rdd.zipWithIndex().toDF()\n",
    "DIM_DATE = df_temp.select(col(\"_1.*\"),col(\"_2\").alias('date_id'))\n",
    "DIM_DATE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearranging the columns according to the target model\n",
    "DIM_DATE = DIM_DATE.select('date_id', 'full_date_time', 'year', 'month', 'day', 'hour', 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all required columns are present and named correctly\n",
    "DIM_DATE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e061c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df card_type and selecting required columns with distinct values\n",
    "card_type = df.select('card_type').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the primary key column\n",
    "df_temp = card_type.rdd.zipWithIndex().toDF()\n",
    "DIM_CARD_TYPE = df_temp.select(col(\"_1.*\"),col(\"_2\").alias('card_type_id'))\n",
    "DIM_CARD_TYPE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eadfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearranging the columns according to the target model\n",
    "DIM_CARD_TYPE = DIM_CARD_TYPE.select('card_type_id', 'card_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99104d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all required columns are present and named correctly\n",
    "DIM_CARD_TYPE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating the count of the dataframe\n",
    "DIM_CARD_TYPE.select('*').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bee389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the colums as required\n",
    "fact_loc = df.withColumnRenamed('atm_location','location').withColumnRenamed('atm_streetname','streetname').withColumnRenamed('atm_street_number','street_number').withColumnRenamed('atm_zipcode','zipcode').withColumnRenamed('atm_lat','lat').withColumnRenamed('atm_lon','lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the df\n",
    "fact_loc = fact_loc.join(DIM_LOCATION, on = ['location', 'streetname', 'street_number', 'zipcode', 'lat', 'lon'], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the columns\n",
    "fact_loc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ba053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the colums as required\n",
    "fact_loc = fact_loc.withColumnRenamed('atm_id', 'atm_number').withColumnRenamed('location_id', 'atm_location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7da094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the df\n",
    "fact_atm = fact_loc.join(DIM_ATM, on = ['atm_number', 'atm_manufacturer', 'atm_location_id'], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing necessary transformation\n",
    "fact_atm = fact_atm.withColumnRenamed('atm_location_id', 'weather_loc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the df\n",
    "fact_date = fact_atm.join(DIM_DATE, on = ['year', 'month', 'day', 'hour', 'weekday'], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a17caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the count of the df at the end of Stage 4\n",
    "fact_atm_trans.select('*').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating primary key of fact table and viewing 1st record of the table\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().orderBy('date_id')\n",
    "FACT_ATM_TRANS = fact_atm_trans.withColumn(\"trans_id\", row_number().over(w))\n",
    "FACT_ATM_TRANS.show(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e459bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the list of columns\n",
    "FACT_ATM_TRANS.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting and arranging only the required columns according to the target model\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.select('trans_id', 'atm_id', 'weather_loc_id', 'date_id', 'card_type_id', \n",
    "'atm_status', 'currency', 'service', 'transaction_amount', 'message_code', 'message_text', 'rain_3h', \n",
    "'clouds_all', 'weather_id', 'weather_main', 'weather_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d77d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that all required columns are present and named correctly\n",
    "FACT_ATM_TRANS.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762491b",
   "metadata": {},
   "source": [
    "AWS S3 storage in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da273ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing data from pyspark df 'dim_location' in csv format to dim_location folder in S3 bucket 'etlprojectrohit'\n",
    "DIM_LOCATION.coalesce(1).write.format('csv').option('header','false').save('s3a://etlprojectrohit/dim_location', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb316ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing data from pyspark df 'dim_atm' in csv format to dim_atm folder in S3 bucket 'etlprojectrohit'\n",
    "DIM_ATM.coalesce(1).write.format('csv').option('header','false').save('s3a://etlprojectrohit/dim_atm', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f07bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing data from pyspark df 'dim_data' in csv format to dim_data folder in S3 bucket 'etlprojectrohit'\n",
    "DIM_DATE.coalesce(1).write.format('csv').option('header','false').save('s3a://etlprojectrohit/dim_date', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing data from pyspark df 'dim_card_type' in csv format to dim_card_type folder in S3 bucket 'etlprojectrohit'\n",
    "DIM_CARD_TYPE.coalesce(1).write.format('csv').option('header','false').save('s3a://etlprojectrohit/dim_card_type', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a503acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing data from pyspark df 'fact_atm_trans' in csv format to fact_atm_trans folder in S3 bucket 'etlprojectrohit'\n",
    "FACT_ATM_TRANS.coalesce(1).write.format('csv').option('header','false').save('s3a://etlprojectrohit/fact_atm_trans', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
